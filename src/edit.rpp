import rl;
import theming;
import timer;
import thread;
import element;
import ui_elements;
import maths;
import data;
import keyframing;

import std;
import map;

import dylib;
import code_manager;
import script_interface;

import yaml;
import global_settings;

c:import "rlgl.h";
c:import "tinyfiledialogs.h";
c:import <"pthread.h">;

c:import <"libavutil/avassert.h">;
c:import <"libavutil/channel_layout.h">;
c:import <"libavutil/opt.h">;
c:import <"libavutil/imgutils.h">;
c:import <"libavutil/samplefmt.h">;
c:import <"libavutil/mathematics.h">;
c:import <"libavutil/timestamp.h">;
c:import <"libavcodec/avcodec.h">;
c:import <"libavformat/avformat.h">;
c:import <"libswscale/swscale.h">;
c:import <"libswresample/swresample.h">;


// MUTEXES

c:pthread_mutex_t images_lock;
c:pthread_cond_t less;
c:pthread_cond_t more;

// Canvas and window

int window_width = GlobalSettings.get_int("window_width", 1200); // loaded from GlobalSettings
int window_height = GlobalSettings.get_int("window_height", 900);
Vec2 window_dimens = v2(window_width, window_height);

int canvas_width = 1200;
int canvas_height = 900;

RenderTexture canvas;

Texture eye_open_icon;
Texture eye_closed_icon;
Texture warning_icon;
Texture image_icon;

Texture play_icon;
Texture pause_icon;
Texture muted_icon;
Texture unmuted_icon;

// Audio

c:Sound fxMP3;

List<Image> imported_images = .();

List<Image> images = .();
int max_images;

// Video

int frame_rate = 60;

int current_frame = 0;
float current_time = 0;
float max_time = 5;
int max_frames = (max_time * frame_rate) as int;
float time_per_frame = 1.0 / frame_rate;

float STREAM_DURATION = time_per_frame * max_frames;
int STREAM_FRAME_RATE = frame_rate;

enum ApplicationMode {
	Running, Exporting, Paused;
	bool operator:==(Self other) -> this as int == other as int;
}

ApplicationMode mode = .Running;

enum ExportType {
	FFMPEG_CALL, INTERNAL;
	bool operator:==(Self other) -> this as int == other as int;
}

ExportType export_type = ExportType.INTERNAL;

float INIT_VOLUME = 0.6;
float master_volume = INIT_VOLUME;
bool muted = false;

bool is_running() -> mode == ApplicationMode.Running;
bool is_exporting() -> mode == .Exporting;
bool is_paused() -> mode == .Paused;

bool ui_hidden = false;

struct ExportState {
	int total_frames;
	int frames_rendered;
	int frames_written;

	bool is_ffmpegging;
	float start_ffmpeg_time;
}

ExportState make_export_state() -> {
	.total_frames = max_frames,
	.frames_rendered = 0,
	.frames_written = 0,
	.is_ffmpegging = false,
	.start_ffmpeg_time = 0,
};

ExportState export_state;

// TODO: sort elements? (insertion sort) -> sort-by layer
// NOTE: currently we traverse N times for rendering, N = # of layers used
List<Element> elements = .();

List<Layer> layers = .(); // metadata stored per-layer (layers do not directly contain elements!)
// TODO: vid_layers & audio_layers -> combined_layers (negative indices for audio? -> rendering ui purposes?)

List<Data> data_list = .();

int selected_elem_i = 0;


//void ExportFrameThread(void^ arg) {
	//int^ i_arg = arg;
	//int i = *i_arg; // images.size - 1
	//free(arg);

	//while (images.size <= i) { }
	//Image img = images.get(i);
	//c:ImageFlipVertical(^img);
	
	//char^ name = f"out/frames/{i %d05}.png"; // t"" not thread safe!
	//defer free(name);
	//img.Unload();
	//export_state.frames_written++;
//}

// List<TextInput> temp_inputs = .();

struct RepeatingTimer {
	float max;
	float t;

	construct(float max_t) -> { .max = max_t, .t = max_t };
	bool DidRepeatWhileUpdating() {
		t = t - rl.GetFrameTime(); // TODO: deltaTime?
		defer {
			if (t <= 0) { t = max; }
		}

		return t <= 0;
	}
}

RepeatingTimer check_code_timer = .(0.25);

void debug_print_env_config() {
	if (is_dvorak) {
		println("Setting keyboard to Dvorak (via env var)");
	}
}

Texture LoadIcon(char^ path) {
	return LoadTextureFromFile(path);
}

char^ OpenImageFileDialog(char^ title) {
	char^^ filters = malloc(sizeof<char^> * 3);
	filters[0] = "*.png";
	filters[1] = "*.jpg";
	filters[2] = "*.gif";
	defer free(filters);

	c:c:`char const* const* c_filters = (char const* const*) filters;`;

    char^ filePath = c:tinyfd_openFileDialog(
        title,
        "",
        3,
        c:c_filters,
        "image files",
        0
    );
    return (filePath != NULL) ? filePath | "";
}


char^ OpenDataFileDialog(char^ title) {
	char^^ filters = malloc(sizeof<char^> * 3);
	filters[0] = "*.csv";
	defer free(filters);

	c:c:`char const* const* c_filters = (char const* const*) filters;`;

	char^ filePath = c:tinyfd_openFileDialog(
		title,
		"",
		1,
		c:c_filters,
		"data files",
		0
	);
	return (filePath != NULL) ? filePath | "";
}

struct CommandLineArgs {
	char^ open_project;
	char^ save_to_project;

	construct(int argc, char^^ argv) {
		char^ open_project = NULL;
		char^ save_to_project = NULL;

		for (int i in 0..argc) {
			if (str_eq("-o", argv[i])) {
				if (i+1 >= argc) { panic("-o requires parameter:project-name (local to ./saves/)"); }
				// TODO: asserts backwards (in rpp)!!
				// assert(argc > i + 1,t);
				open_project = argv[i + 1];
			} else if (str_eq("-s", argv[i])) {
				if (i+1 >= argc) { panic("-s requires parameter:project-name (local to ./saves/)"); }
			// println(t"hi: {argc=} {i+1=}");
				// assert(argc <= i + 1, "-s requires parameter:project-name (local to ./saves/)");
				save_to_project = argv[i + 1];
			}
		}

		return {
			:open_project,
			:save_to_project,
		};
	}
}

struct ProjectSave {
	static void Load(Path project_dir_path) {
		elements = .(); // clear elements
		selected_elem_i = 0;
		
		let obj = yaml_parser{}.parse_file(project_dir_path/"manifest.yaml");

		char^ project_name = obj.get_str("project_name");
		char^ edit_version = obj.get_str("edit_version");
		int num_elements = obj.get_int("num_elements");

		// obj.delete();

		let layers_list_obj = yaml_parser{}.parse_file(project_dir_path/"layers.yaml");

		layers = .();
		for (int i in 0..layers_list_obj.list.size) {
			let& layer_obj = layers_list_obj.at_obj(i);
			layers.add(Layer{
				.visible = layer_obj.get_bool("visible"), // TODO: get_bool
			});
		}

		for (int i in 0..num_elements) {
			// TODO: free RectElement!
			AddNewElementAt(Element(RectElement.Make(), "basic_element_for_overwrite", 0, 1, -1, v2(0, 0), v2(200, 150)));
			elements.get(i).Serialize(project_dir_path/t"elements/{i}.yaml", true);
		}
	}

	static void Create(Path project_dir_path, char^ name) {
		io.mkdir(project_dir_path);

		yaml_object manifest = make_yaml_object();
		manifest.put_literal("project_name", name);
		manifest.put_literal("edit_version", "0.0.1");
		manifest.put_int("num_elements", elements.size);

		manifest.serialize_to(project_dir_path/"manifest.yaml");

		yaml_object layers_obj = make_yaml_object(); // TODO: free
		for (let& layer in layers) {
			let layer_obj = make_yaml_object();
			layer_obj.put_bool("visible", layer.visible);
			layers_obj.push_object(layer_obj);
		}
		layers_obj.serialize_to(project_dir_path/"layers.yaml");

		io.mkdir(project_dir_path/"elements");

		for (int i in 0..elements.size) {
			elements.get(i).Serialize(project_dir_path/t"elements/{i}.yaml", false);
		}

		// yo.push_object({
		// 	
		// });
	}
}

int main(int argc, char^^ argv) {
	defer GlobalSettings.SaveUpdates();

	CommandLineArgs args = .(argc, argv);
	debug_print_env_config();

	code_man.PreLoadTakeCareOfPreppedReload();
	code_man.Load();
	defer code_man.Unload();

	defer ImageCache.Unload(); // cleanup loaded images (we should also do this when assets are no longer in use? -- TODO: LCS eviction type thing maybe)

	// TODO: println(t"{test.get("apple")}"); (quote properly when inserting into talloc!!)

	rl.SetTraceLogLevel(c:LOG_WARNING ~| c:LOG_ERROR);  // Log flags? -- comment out to see if I/O or shader stuff is working!
	// TODO: high dpi -> increase sizes of render textures!
	rl.SetConfigFlags(c:FLAG_MSAA_4X_HINT ~| c:FLAG_WINDOW_RESIZABLE ~| c:FLAG_WINDOW_HIGHDPI);      // Enable Multi Sampling Anti Aliasing 4x (if available)

	// RAYLIB INITIALIZED HERE (window.init), no loading assets (textures, images, sounds) before this point!!!
	window.Init(window_width, window_height, f"CodeComposite{(args.save_to_project != NULL) ? t" - {args.save_to_project}" | ""}"); // TODO: mem leak?
	defer window.Close();

	// Audio init and close
	c:InitAudioDevice();
	defer c:CloseAudioDevice();
	fxMP3 = c:LoadSound("assets/history.mp3");
	defer c:UnloadSound(fxMP3);

	c:SetMasterVolume(master_volume);
	c:PlaySound(fxMP3);

	SetMute(true); // NOTE: CURRENTLY MUTING FOR DEMO

	canvas = make_render_texture(1200, 900);

	eye_open_icon = LoadIcon(t"assets/eye_open.png");
	eye_closed_icon = LoadIcon(t"assets/eye_closed.png");
	warning_icon = LoadIcon(t"assets/warning_dark.png");
	image_icon = LoadIcon(t"assets/image.png");

	play_icon = LoadIcon(t"assets/play.png");
	pause_icon = LoadIcon(t"assets/pause.png");
	muted_icon = LoadIcon(t"assets/muted.png");
	unmuted_icon = LoadIcon(t"assets/unmuted.png");

	images.reserve(max_frames);

	AddNewElementAt(Element(RectElement.Make(), "Rect", 0, 1, -1, v2(0, 0), v2(200, 150)) with { color = Colors.Blue });
	// AddNewElementAt(Element(CustomPureFnElement.Make("perlin_field"), "perlin_field", 1, 2, -1, v2(0, 0), v2(100, 100)));
	// AddNewElementAt(Element(CustomPureFnElement.Make("nonexistent"), "nonexistent", 1, 0.5, -1, v2(0, 0), v2(100, 100)));
	// AddNewElementAt(Element(CustomPureFnElement.Make("cool_effect"), "cool_effect", 3, 2, -1, v2(0, 0), v2(100, 100)));
	// AddNewElementAt(Element(CustomPureFnElement.Make("PointSwarm"), "PointSwarm", 0, 3, -1, v2(0, 0), v2(100, 100)));
	AddNewElementAt(Element(CustomPureFnElement.Make("StringWheel"), "StringWheel", 0, 3, -1, v2(0, 0), v2(100, 100)));

	// AddNewElementAt(Element(CustomPureFnElement.Make("MyFx"), "MyFx", 0, 2, -1, v2(0, 0), v2(100, 100)));
	// AddNewElementAt(Element(CustomPureFnElement.Make("MyFx2"), "MyFx2", 2, 2, -1, v2(0, 0), v2(100, 100)));
	// AddNewElementAt(Element(CustomPureFnElement.Make("bar_chart"), "bar_chart", 0, 5, -1, v2(100, 100), v2(1000, 500)) with { color = Colors.Blue });

	selected_elem_i = 0;

	// temp_inputs.add(TextInput.make("a"));
	// temp_inputs.add(TextInput.make("what"));
	// temp_inputs.add(TextInput.make(""));
	// for (let p in temp_inputs.pairs()) {
	// 	p.value.Layout(v2(10, 100 + 30 * p.key), 200);
	// }

	if (args.open_project != NULL) {
		println(t"opening: saves/{args.open_project}");
		ProjectSave.Load(Path("saves")/args.open_project);
	}

	rl.SetTargetFPS(60);
	while (!window.ShouldClose()) {

		// File drop listener
		if (c:IsFileDropped()) {
			OnFileDropped();
		}

		d.Begin();
		GameTick();
		d.End();
		tfree(); // not thread safe (don't use temp-alloc from other threads!) ----------
    }

	if (args.save_to_project != NULL) {
		ProjectSave.Create(Path("saves")/args.save_to_project, args.save_to_project);
	}

    return 0;
}

// adds element, adding it's corresponding layer if necessary (NOTE: will create up to N layers, if placed at layer N...)
// NOTE: direct use discouraged, generally use AddNewElementAt, since that won't cause overlap issues
void AddLayersTill(int layer) {
	while (layer >= layers.size) {
		layers.add({
			.visible = true
		});
	}
}

void AddNewElement(Element elem) {
	AddLayersTill(elem.layer);
	elements.add(elem);
}

// gets first layer at which there wouldn't be a collision if an element at this time/length were added!
int FirstAvailableLayerAt(float start_time, float duration) {
	for (int i in 0..layers.size) {
		bool collision = false;
		for (let& elem in elements) { // TODO: don't loop over all elements... (just the ones on the right layer pls :D)
			if (elem.layer == i && elem.CollidesWith(start_time, duration)) {
				collision = true;
				break;
			}
		}
		if (!collision) { return i; }
	}
	return layers.size; // will have to add 1 new layer to accomodate element placement
}

// sets elem to correct layer (NOTE: pass with layer=-1, for clarity)
void AddNewElementAt(Element elem) {
	AddNewElement(elem with { layer = FirstAvailableLayerAt(elem.start_time, elem.duration) });
}

void SelectNewestElement() {
	selected_elem_i = elements.size - 1;
}

float new_element_default_duration = 0.5;

bool look_at_controls = false;
bool show_add_element_options = false;

int left_panel_width = GlobalSettings.get_int("left_panel_width", 300);
void LeftPanelUI() {
	d.Rect(v2(0, 0), v2(left_panel_width, window_height), theme.panel);
	d.Rect(v2(left_panel_width, 0), v2(1, window_height), theme.panel_border);

	// Elements List
	Vec2 btn_dimens = v2(left_panel_width - 20, 40);
	// for (let& input in temp_inputs) { // TODO: gd doesn't work on refs!
	// 	input.Do();
	// }

	if (show_add_element_options) {
		Vec2 options_tl = v2(10, window_height - 170);
		Vec2 options_dims = v2(left_panel_width - 20, 100);
		d.RectR(Rectangle.FromV(options_tl, options_dims).Pad(6), theme.button);

		// Add media file
		Vec2 icon_dimens = v2((left_panel_width - 40) / 3, 80);
		Vec2 icon_tl = options_tl + v2(5, 5);
		if (Button(icon_tl, icon_dimens, "")) {
			char^ file_path = OpenImageFileDialog("Select an image");
			if (file_path != NULL) {
				Image img = Image.Load(file_path);
				defer img.Unload();

				char^ img_name = c:GetFileNameWithoutExt(file_path);
				AddNewElementAt(Element(ImageElement.Make(file_path), strdup(img_name), current_time, new_element_default_duration, -1, v2(0, 0), v2(img.width, img.height)));
				SelectNewestElement();
				show_add_element_options = false;
			}
		}
		d.TextureAtSizeV(image_icon, icon_tl, icon_dimens);

		// Add data
		icon_tl = icon_tl + v2((left_panel_width - 40) / 3, 0);
		if (Button(icon_tl, icon_dimens, "{data}")) {
			char^ file_path = OpenDataFileDialog("Select a data file");
			println(t"Selected data file: {file_path}");
			if (strlen(file_path) > 0) {
				// Load data and apply to keyframe
				Data data = Data(file_path);
				// elements.get(selected_elem_i).ApplyKeyframeData(data);
				elements.get(selected_elem_i).ApplyData(Box<Data>.Make(data));
				data_list.add(data);
				show_add_element_options = false;
			}
		}

		// Add arbitrary element (Temporary)
		Vec2 plus_tl = options_tl + v2((left_panel_width - 10) / 3 * 2, 5);
		if (Button(plus_tl, v2((left_panel_width - 40) / 3, 80), "+")) {
			AddNewElementAt(Element(CircleElement.Make(), NULL, current_time, new_element_default_duration, -1, v2(0, 0), v2(100, 100)) with { color = Colors.Green });
			SelectNewestElement();
		}
	}
	
	// Add Element Button
	Vec2 tl = v2(10, window_height - 50);
	if (Button(tl, btn_dimens, "+")) {
		show_add_element_options = true;
	} else if (mouse.LeftClickPressed()) {
		show_add_element_options = false;
	}

	d.RectR(.(0, 0, left_panel_width, 32), theme.button);
	let& selected_elem = elements.get(selected_elem_i);
	d.Text(t"Selected: {selected_elem.name}", 5, 5, 20, Colors.RayWhite);
	KeyframeTimelineUI();
}

struct TimelineState {
	bool dragging_elem;
	bool dragging_elem_start;
	bool dragging_elem_end;

	bool is_dragging_elem_any() -> dragging_elem || dragging_elem_end || dragging_elem_start;

	float elem_drag_init_mouse_x;
	float elem_drag_init_start;
	float elem_drag_init_end;

	bool dragging_caret;
}
TimelineState timeline = {
	.dragging_elem = false,
	.dragging_elem_start = false,
	.dragging_elem_end = false,
	.elem_drag_init_mouse_x = 0,
	.elem_drag_init_start = 0,
	.elem_drag_init_end = 0,
	.dragging_caret = false,
};

int keyframe_timeline_ui_height = 100;
int element_timeline_ui_height = 180;
void ElementTimelineUI() {
	float timeline_view_start = 0;
	float timeline_view_duration = max_time;

	int show_layers = std.maxi(layers.size + 1, 3);
	float height = element_timeline_ui_height;
	float layer_height = height / show_layers;

	float whole_width = window_width - left_panel_width;
	Vec2 whole_tl = v2(left_panel_width + 1, window_height as float - height);
	Vec2 whole_dimens = v2(whole_width, height);
	Rectangle whole_rect = Rectangle.FromV(whole_tl, whole_dimens);
	d.RectR(whole_rect, theme.panel);

	float info_width = 32;

	for (int i in 0..show_layers) { // empty skeleton for layers
		float x = whole_tl.x;
		float y = whole_rect.b() - (i + 1) as float * layer_height;
		Rectangle r = .(x, y, info_width, layer_height);

		d.RectR(r.Inset(1), theme.button);
		d.RectR(.(x, y, whole_width, 1), theme.panel_border);

		if (layers.size > i) {
			// layer info ui -------
			let& layer = layers.get(i);

			if (Button(r.tl(), r.dimen(), "")) {
				layer.visible = !layer.visible;
			}

			d.Text(t"L{i}", x as int + 6, y as int + 6, 12, theme.timeline_layer_info_gray);

			if (layer.visible) {
				d.TextureAtRect(eye_open_icon, r.Inset(6).FitIntoSelfWithAspectRatio(1, 1));
			}
			// ---------------------
		}
	}

	Vec2 tl = whole_tl + v2(info_width, 0);
	Vec2 dimens = whole_dimens - v2(info_width, 0);

	bool pressed_inside = mouse.LeftClickPressed() && mouse.GetPos().InV(tl, dimens);

	float width = dimens.x;

	for (int i in 0..elements.size) {
		let& elem = elements.get(i);
		// elem.TimelineUI();
		float x = tl.x + (elem.start_time - timeline_view_start) / timeline_view_duration * width;
		float y = whole_rect.b() - (elem.layer + 1) as float * layer_height;

		float w = elem.duration / timeline_view_duration * width;

		Rectangle r = .(x, y, w, layer_height);

		bool has_err = elem.err_msg != NULL;
		TimelineElementColorSet color_set = (selected_elem_i == i) ? theme.elem_ui_blue | 
			(has_err ? theme.elem_ui_yellow | theme.elem_ui_pink);
		// selected -> blue
		// warning -> yellow
		// otherwise -> pink

		d.RectR(r, color_set.border);
		d.RectR(r.Inset(1), color_set.bg);

		d.Text(elem.name, x as int + 6, y as int + 6, 12, color_set.text);
		d.Text(elem.content_impl#ImplTypeStr(), x as int + 6, (y + layer_height) as int - 18, 12, color_set.text);

		bool hovering = mouse.GetPos().Between(r.tl(), r.br());
		if (has_err) {
			Vec2 warning_dimen = v2(16, 16);
			d.TextureAtSizeV(warning_icon, r.br() - warning_dimen - v2(6, 6), warning_dimen);

			if (hovering) {
				Vec2 options_tl = mouse.GetPos() + v2(0, 10);
				Vec2 options_dims = c:MeasureTextEx(c:GetFontDefault(), elem.err_msg, 16, 1);
				d.RectR(Rectangle.FromV(options_tl, options_dims).Pad(6), theme.button);
				d.Text(elem.err_msg, options_tl.x as.., options_tl.y as.., 16, Colors.White);
			}
		}

		if (hovering && mouse.LeftClickPressed()) {
			selected_elem_i = i;

			if (mouse.GetPos().Between(r.tl(), r.tl() + v2(10, layer_height))) {
				timeline.dragging_elem_start = true;
			} else if (mouse.GetPos().Between(r.br() - v2(10, layer_height), r.br())) {
				timeline.dragging_elem_end = true;
			} else {
				timeline.dragging_elem = true;
			}
			timeline.elem_drag_init_mouse_x = mouse.GetPos().x;
			timeline.elem_drag_init_start = elem.start_time;
			timeline.elem_drag_init_end = elem.end_time();
		}
	}

	d.Rect(whole_rect.bl(), v2(dimens.x, 1), theme.panel_border); // TODO: change

	d.Rect(tl + v2(dimens.x * current_time / max_time, 0), v2(1, dimens.y), theme.active);

	// mouse interactions --------------------
	if (pressed_inside && !timeline.is_dragging_elem_any()) {
		timeline.dragging_caret = true;
	}

	if (mouse.LeftClickDown()) {
		float t_on_timeline_unbounded = (mouse.GetPos().x - tl.x) / dimens.x * max_time;
		float t_on_timeline_bounded = std.clamp(t_on_timeline_unbounded, 0, max_time);
		if (timeline.dragging_caret) {
			float new_time = (mouse.GetPos().x - tl.x) / dimens.x * max_time;
			if (new_time <= 0) { new_time = 0; }
			if (new_time >= max_time) { new_time = max_time; }
			SetTime(new_time); // TODO: add snap-to-frame-set-time
			SetFrame(current_frame);
		} else if (timeline.dragging_elem) {
			float og_t_on_timeline = (timeline.elem_drag_init_mouse_x - tl.x) / dimens.x * max_time;
			let& selected_elem = elements.get(selected_elem_i);
			selected_elem.start_time = SnapToNearestFramesTime(timeline.elem_drag_init_start + (t_on_timeline_unbounded - og_t_on_timeline));
			selected_elem.start_time = std.max(0, selected_elem.start_time);

			selected_elem.layer = (((whole_rect.b() - mouse.GetPos().y) / layer_height) as int);
			AddLayersTill(selected_elem.layer);
		} else if (timeline.dragging_elem_start) {
			float og_t_on_timeline = (timeline.elem_drag_init_mouse_x - tl.x) / dimens.x * max_time;
			let& selected_elem = elements.get(selected_elem_i);
			selected_elem.start_time = SnapToNearestFramesTime(timeline.elem_drag_init_start + (t_on_timeline_unbounded - og_t_on_timeline));
			selected_elem.start_time = std.max(0, selected_elem.start_time);
			selected_elem.duration = (timeline.elem_drag_init_end - timeline.elem_drag_init_start) - (selected_elem.start_time - timeline.elem_drag_init_start);
		} else if (timeline.dragging_elem_end) {
			float og_t_on_timeline = (timeline.elem_drag_init_mouse_x - tl.x) / dimens.x * max_time;
			let& selected_elem = elements.get(selected_elem_i);
			selected_elem.duration = SnapToNearestFramesTime((timeline.elem_drag_init_end - timeline.elem_drag_init_start) + (t_on_timeline_unbounded - og_t_on_timeline));
		}
	}

	if (!mouse.LeftClickDown()) {
		timeline.dragging_caret = false;
		timeline.dragging_elem = false;
		timeline.dragging_elem_start = false;
		timeline.dragging_elem_end = false;

		CullEmptyLayers();
	}
}

bool keyframe_timeline_dragging = false;
void KeyframeTimelineUI() {
	int info_width = 100;
	// int width = window_width - left_panel_width - info_width;
	int width = left_panel_width;
	int height = keyframe_timeline_ui_height;

	int y_start = 32;
	Vec2 info_tl = v2(0, y_start);

	Vec2 tl = v2(info_width, y_start);
	Vec2 dimens = v2(width - info_width, height);


	//  info -------------------
	d.Rect(info_tl, v2(info_width, height), theme.panel);
	// d.Rect(tl + v2(info_width - 1, 0), v2(1, height), theme.panel_border);
	// /info -------------------

	//  ui -------------------
	d.Rect(tl, dimens, theme.panel);
	d.Rect(tl, v2(dimens.x, 1), theme.panel_border);

	let& selected_elem = elements.get(selected_elem_i);

	d.Rect(tl + v2(dimens.x * std.clamp((current_time - selected_elem.start_time) / selected_elem.duration, 0, 1), 0), v2(1, dimens.y), theme.active);

	int i = 0;


	int kl_height = height / 5;
	Vec2 kl_dimens = v2(dimens.x, kl_height);
	float max_elem_time = selected_elem.duration;

	float curr_lt = current_time - selected_elem.start_time;
	KeyframeLayerUI_Float(selected_elem.kl_pos_x, tl, kl_dimens, max_elem_time, curr_lt, "x", i); i++;
	KeyframeLayerUI_Float(selected_elem.kl_pos_y, tl, kl_dimens, max_elem_time, curr_lt, "y", i); i++;
	KeyframeLayerUI_Float(selected_elem.kl_scale, tl, kl_dimens, max_elem_time, curr_lt, "scale", i); i++;
	KeyframeLayerUI_Float(selected_elem.kl_rotation, tl, kl_dimens, max_elem_time, curr_lt, "angle", i); i++;
	KeyframeLayerUI_Float(selected_elem.kl_opacity, tl, kl_dimens, max_elem_time, curr_lt, "opacity", i); i++;

	if (selected_elem.content_impl#CustomLayers() != NULL) {
		// println(t"ui for {selected_elem.content_impl#CustomLayers()#size}");
		for (let& layer in *selected_elem.content_impl#CustomLayers()) {
			// println(t"ui for {layer.name}");
			// layer.kl_value.UI(tl + v2(0, i * kl_height), kl_dimens, max_elem_time, curr_lt, layer.name); i++;
			layer.UI(tl, kl_dimens, max_elem_time, curr_lt, layer.name, i);
		}
	}

	if (mouse.LeftClickPressed() && mouse.GetPos().InV(tl, dimens)) {
		keyframe_timeline_dragging = true;
	}

	if (mouse.LeftClickDown() && keyframe_timeline_dragging) {
		float new_time = std.clamp(((mouse.GetPos().x - tl.x) / dimens.x), 0, 1) * selected_elem.duration + selected_elem.start_time;
		if (new_time <= 0) { new_time = 0; }
		if (new_time >= max_time) { new_time = max_time; }
		SetTime(new_time); // TODO: add snap-to-frame-set-time
		SetFrame(current_frame);
	}

	if (!mouse.LeftClickDown()) {
		keyframe_timeline_dragging = false;
	}
	//  /ui -------------------
}

float SnapToNearestFramesTime(float time) -> ((time / time_per_frame) as int) as float / frame_rate;

void SetFrame(int frame) {
	current_time = time_per_frame * frame;
	current_frame = frame;
	UpdateState();
}

void SetTime(float new_time) {
	current_time = new_time;
	current_frame = (current_time / time_per_frame) as int;
	UpdateState();
}

bool ElementIsVisibleNow(Element& elem) {
	return elem.visible && elem.ActiveAtTime(current_time) && layers.get(elem.layer).visible;
}

void UpdateState() {
	for (let& elem in elements) {
		if (ElementIsVisibleNow(elem)) { // QUESTION: should we update non-rendered elements? - currently I say no (may change if we add element dependencies!)
			elem.UpdateState(current_time);
		}
	}
}

void DrawFrameToCanvas() {
	canvas.Begin();
	d.ClearBackground(theme.bg);
	for (int i in 0..layers.size) {
		for (let& elem in elements) {
			if (elem.layer == i && ElementIsVisibleNow(elem)) {
				elem.Draw();
			}
		}
	}

	if (!is_exporting()) {
		elements.get(selected_elem_i).DrawGizmos();
	}

	canvas.End();
}

void ExportVideoThread() {
	ExportVideo(frame_rate, "out", "edit_video");

	SetMode(.Paused);
	SetFrame(0);
}

void DrawProgressBar(Vec2 tl, Vec2 dimens, Color bg, Color fg, float amount) {
	d.Rect(tl, dimens, bg);
	d.Rect(tl, dimens * v2(amount, 1), fg);
}

void DrawExportProgressOverlay() {
	Color shadow = hex("00000077");
	d.Rect(v2(0, 0), window_dimens, shadow);

	float width = 0.5 * window_width;
	float height = 20 * 5 + 2 * 10;
	Vec2 tl = window_dimens * v2(0.25, 0.5) - v2(0, height / 2);
	Vec2 dimens = v2(width, height);
	d.Rect(tl - v2(4, -1), dimens + v2(8, 3), hex("00000033"));
	d.Rect(tl - v2(1, 1), dimens + v2(2, 2), Colors.Black);
	d.Rect(tl, dimens, theme.panel);

	Color progress_bg = hex("262626"); // gray
	Color progress_fg = hex("afc7af"); // green

	Vec2 pbar_tl = v2(tl.x + 20, tl.y + 20);
	Vec2 pbar_dimens = v2(width - 40, 20);

	DrawProgressBar(pbar_tl, pbar_dimens, progress_bg, progress_fg, (export_state.frames_rendered) as float / export_state.total_frames);

	pbar_tl.y = pbar_tl.y + 30;
	DrawProgressBar(pbar_tl, pbar_dimens, progress_bg, progress_fg, (export_state.frames_written) as float / export_state.total_frames);

	pbar_tl.y = pbar_tl.y + 30;
	Color ffmpeg_load_pulse_color = 
		export_state.is_ffmpegging
			? ColorLerp(progress_bg, progress_fg, Sin01((rl.GetTime() - export_state.start_ffmpeg_time) * 5) * 0.7)
			| progress_bg;
	d.Rect(pbar_tl, pbar_dimens, ffmpeg_load_pulse_color);
}

bool is_dvorak = c:getenv("SILLY_DVORAK_USER") != NULL;
struct HotKey {
	int key_code;
	// TODO: modifiers

	bool IsPressed() -> NoTextInputFocused() && key.IsPressed(key_code);

	static Self DvoKey(int keycode_qwerty, int keycode_dvorak) -> {
		.key_code = is_dvorak ? keycode_dvorak | keycode_qwerty 
	};
	static Self Key(int keycode) -> Self.DvoKey(keycode, keycode);
}

struct HotKeys {
	// :hotkey

	static HotKey PlayPause = HotKey.Key(KEY.SPACE);

	static HotKey Mute = HotKey.Key(KEY.M);

	static HotKey ExportMovie = HotKey.DvoKey(KEY.E, KEY.D);

	// key as in keyframe
	static HotKey KeyAtCurrentPosition  = HotKey.DvoKey(KEY.K, KEY.V);
	static HotKey Alternative_KeyAtCurrentPosition  = HotKey.DvoKey(KEY.A, KEY.A);

	static HotKey ToggleHideUIFullscreenPlayback  = HotKey.DvoKey(KEY.H, KEY.J);

	static HotKey QuickCut = HotKey.DvoKey(KEY.B, KEY.N); // contextual delete? like blender? // AT LEAST: remove Keys at current pos?

	// :hotkey:temp
	static HotKey Temp_ClearTimeline = HotKey.DvoKey(KEY.C, KEY.I);
	static HotKey Temp_DeleteElement = HotKey.DvoKey(KEY.X, KEY.B);
	static HotKey Temp_ReloadCode = HotKey.DvoKey(KEY.R, KEY.O); // TODO: make this do something else -- since we reload?

	static HotKey Temp_AddElementCircle = HotKey.DvoKey(KEY.O, KEY.S);
	static HotKey Temp_AddElementCool = HotKey.DvoKey(KEY.N, KEY.L);

	static HotKey Temp_LeftSidebar_Less = HotKey.DvoKey(KEY.Y, KEY.T);
	static HotKey Temp_LeftSidebar_More = HotKey.DvoKey(KEY.F, KEY.Y);
}

void SetMute(bool mute) {
	if (mute) {
		c:SetMasterVolume(0.0);
	} else {
		c:SetMasterVolume(master_volume);
	}
	muted = mute;
}

void SetMode(ApplicationMode new_mode) {
	mode = new_mode;
	switch (new_mode) { // mode change side effects
		.Running -> {
			c:ResumeSound(fxMP3);
		},
		else -> {
			c:PauseSound(fxMP3);
		},
	}
}

void CullEmptyLayers() {
	for (int i = layers.size - 1; i >= 0; i--;) {
		for (let& elem in elements) {
			if (elem.layer == i) { return; }
		}
		layers.pop_back();
	}
}

Vec2 GetMousePosWorldSpace() {
	let canvas_rect = CanvasRect();
	return (mouse.GetPos() - canvas_rect.tl()) / (canvas_rect.dimen() / window_dimens);
}

void UpdateWindowSize(int width, int height) {
	window_width = GlobalSettings.set_int("window_width", width);
	window_height = GlobalSettings.set_int("window_height", height);
	window_dimens = v2(width, height);
}

bool init = false;
void GameTick() {
	ui_element_activated_this_frame = false;
	if (window_width != rl.GetScreenWidth() || window_height != rl.GetScreenHeight()) {
		UpdateWindowSize(rl.GetScreenWidth(), rl.GetScreenHeight());
	}

	mp_world_space = GetMousePosWorldSpace();

	if (check_code_timer.DidRepeatWhileUpdating()) {
		code_man.CheckModifiedTimeAndReloadIfNecessary();
	}

	// d.ClearBackground(theme.bg);
	d.ClearBackground(Colors.Black);

	if (HotKeys.PlayPause.IsPressed()) {
		if (is_running()) {
			SetMode(.Paused);
			SetFrame(current_frame);
		} else if (is_paused()) {
			SetMode(.Running);
		}
	}

	if (HotKeys.Mute.IsPressed()) {
		SetMute(!muted);
	}

	// export movie
	if (HotKeys.ExportMovie.IsPressed()) { // NOTE: E (export)
		SetMode(.Exporting);
		export_state = make_export_state();
		SetFrame(0);
	}

	if (HotKeys.ToggleHideUIFullscreenPlayback.IsPressed()) {
		ui_hidden = !ui_hidden;
	}

	if (elements.size > 1 && HotKeys.Temp_DeleteElement.IsPressed()) {
		elements.remove_at(selected_elem_i);
		CullEmptyLayers();
		if (selected_elem_i == elements.size) { selected_elem_i--; }
	}

	if (HotKeys.Temp_ClearTimeline.IsPressed()) {
		elements.get(selected_elem_i).ClearTimelinesCompletely();
	}

	// if (HotKeys.Temp_ReloadCode.IsPressed()) {
	// 	code_man.Reload();
	// }

	// if (HotKeys.Temp_AddElementCool.IsPressed()) {
	// 	elements.add(make_cool_fn_element());
	// }
	//
	// if (HotKeys.Temp_AddElementCircle.IsPressed()) {
	// 	elements.add(make_element());
	// }

	// :hotkey:use:global

	// if (!is_paused()) {
	UpdateState();
	// }

	DrawFrameToCanvas();

	if (is_running()) {
		float new_time = current_time + rl.GetFrameTime();
		if (new_time > max_time) { new_time = 0; }
		SetTime(new_time);
	} else if (is_exporting()) {
		if (export_state.frames_rendered < max_frames) {
			// Images of format type: PIXELFORMAT_UNCOMPRESSED_R8G8B8A8 == 7 (by raylib.h)
			// Appears equivalent to PIX_FMT_RGBA of FFMPEG
			Image img = c:LoadImageFromTexture(canvas.texture);
			if (export_type == ExportType.INTERNAL) {
				c:ImageFlipVertical(^img);
				c:c:`
				//pthread_mutex_lock(&images_lock);
				//while (images.size >= images.capacity) {
				//	pthread_cond_wait(&less, &images_lock);
				//}
				`;
				images.add(img);
				c:c:`
				//pthread_cond_signal(&more);
				//pthread_mutex_unlock(&images_lock);
				`;

				export_state.frames_rendered++;
				SetFrame(current_frame + 1);
			}
			
		}
		if (!export_state.is_ffmpegging && export_state.frames_rendered == max_frames) {
			export_state.is_ffmpegging = true;
			export_state.start_ffmpeg_time = c:GetTime();
			go(ExportVideoThread);
		}
	}

	let canvas_rect = CanvasRect();
	d.TextureAtRect(canvas, canvas_rect);
	d.RectOutlineR(canvas_rect, theme.bg);

	if (!is_exporting()) {
		if (mouse.LeftClickPressed()) {
			// TODO:(toby) go down in layer!!!, not by i
			for (int i = elements.size - 1; i >= 0; i--;) {
				if (ElementIsVisibleNow(elements.get(i)) && elements.get(i).Hovered()) {
					selected_elem_i = i;
					break;
				}
			}
		}

		if (HotKeys.KeyAtCurrentPosition.IsPressed() || HotKeys.Alternative_KeyAtCurrentPosition.IsPressed()) { // NOTE: K (keyframe)
			let& elem = elements.get(selected_elem_i);
			float keyframe_t = std.clamp(current_time - elem.start_time, 0, elem.duration);
			elem.kl_pos_x.InsertValue(
				.time = keyframe_t,
				.value = mp_world_space.x
			);
			elem.kl_pos_y.InsertValue(
				.time = keyframe_t,
				.value = mp_world_space.y
			);
		}

		if (HotKeys.Mute.IsPressed()) {
			c:c:`
			char video_path[256];
			printf("\nInput file path, include extension: \n>");
			scanf("%s", video_path);
			`;
			ImportVideo(frame_rate, "out", c:video_path);
		}
	}

	if (HotKeys.Temp_LeftSidebar_Less.IsPressed()) {
		left_panel_width = GlobalSettings.set_int("left_panel_width", left_panel_width - 50);
	}

	if (HotKeys.Temp_LeftSidebar_More.IsPressed()) {
		left_panel_width = GlobalSettings.set_int("left_panel_width", left_panel_width + 50);
	}

	if (!ui_hidden) {
		LeftPanelUI();
		ElementTimelineUI();
	}
	VideoPlayPauseMuteUI(PlaceVideoPlayPauseMuteUI()); // TODO: fade out when non-interacted in ui-hidden mode!

	if (is_exporting()) {
		DrawExportProgressOverlay();
	}

	// char^ change_text = TextBox(UiElementID.ID("my_text", 0), "my_text", {
	// 	.x = 0, .y = 500,
	// 	.width = left_panel_width, .height = 24
	// }, 16);
	//
	// if (change_text != NULL) {
	// 	println(t"edited: {change_text=}");
	//
	// 	free(edit_text);
	// 	edit_text = strdup(change_text);
	// }


	if (mouse.LeftClickPressed() && !ui_element_activated_this_frame) {
		focused_ui_elem_id = UiElementID.ID(NULL);
		// println("deactivating");
	}
}
char^ edit_text = f"";

float video_play_pause_mute_ui_height = 32;

Vec2 PlaceVideoPlayPauseMuteUI() {
	let canvas_rect = CanvasRect();
	return v2(canvas_rect.center().x, canvas_rect.b() - (video_play_pause_mute_ui_height / 2 + 10));
}

void VideoPlayPauseMuteUI(Vec2 center_pos) {
	// play/pause (center) | mute
	float b_size = video_play_pause_mute_ui_height;
	Vec2 b_dim = v2(b_size, b_size);
	Vec2 ref = center_pos - b_dim.divide(2);
	Vec2 x_delta = v2(b_size + 10, 0);
	
	if (ButtonIcon(ref, b_dim, mode == .Paused ? play_icon | pause_icon)) {
		SetMode((mode == .Paused) ? ApplicationMode.Running | ApplicationMode.Paused);
	}

	if (ButtonIcon(ref + x_delta, b_dim, muted ? muted_icon | unmuted_icon)) {
		SetMute(!muted);
	}
}

//============= FFMPEG HELPERS ====================================================

c:c:`
#define STREAM_PIX_FMT    AV_PIX_FMT_YUV420P /* default pix_fmt */

#define SCALE_FLAGS SWS_BICUBIC

// a wrapper around a single output AVStream
typedef struct OutputStream {
    AVStream *st;
    AVCodecContext *enc;

    /* pts of the next frame that will be generated */
    int64_t next_pts;
    int samples_count;

    AVFrame *frame;
    AVFrame *tmp_frame;

    AVPacket *tmp_pkt;

    float t, tincr, tincr2;

    struct SwsContext *sws_ctx;
    struct SwrContext *swr_ctx;
} OutputStream;

static void log_packet(const AVFormatContext *fmt_ctx, const AVPacket *pkt)
{
    AVRational *time_base = &fmt_ctx->streams[pkt->stream_index]->time_base;

    printf("pts:%s pts_time:%s dts:%s dts_time:%s duration:%s duration_time:%s stream_index:%d\n",
           av_ts2str(pkt->pts), av_ts2timestr(pkt->pts, time_base),
           av_ts2str(pkt->dts), av_ts2timestr(pkt->dts, time_base),
           av_ts2str(pkt->duration), av_ts2timestr(pkt->duration, time_base),
           pkt->stream_index);
}

static int write_frame(AVFormatContext *fmt_ctx, AVCodecContext *c,
                       AVStream *st, AVFrame *frame, AVPacket *pkt)
{
    int ret;

    // send the frame to the encoder
    ret = avcodec_send_frame(c, frame);
    if (ret < 0) {
        fprintf(stderr, "Error sending a frame to the encoder: %s\n",
                av_err2str(ret));
        exit(1);
    }

    while (ret >= 0) {
        ret = avcodec_receive_packet(c, pkt);
        if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)
            break;
        else if (ret < 0) {
            fprintf(stderr, "Error encoding a frame: %s\n", av_err2str(ret));
            exit(1);
        }

        /* rescale output packet timestamp values from codec to stream timebase */
        av_packet_rescale_ts(pkt, c->time_base, st->time_base);
        pkt->stream_index = st->index;

        /* Write the compressed frame to the media file. */
        log_packet(fmt_ctx, pkt);
        ret = av_interleaved_write_frame(fmt_ctx, pkt);
        /* pkt is now blank (av_interleaved_write_frame() takes ownership of
         * its contents and resets pkt), so that no unreferencing is necessary.
         * This would be different if one used av_write_frame(). */
        if (ret < 0) {
            fprintf(stderr, "Error while writing output packet: %s\n", av_err2str(ret));
            exit(1);
        }
    }

    return ret == AVERROR_EOF ? 1 : 0;
}

/* Add an output stream. */
static void add_stream(OutputStream *ost, AVFormatContext *oc,
                       const AVCodec **codec,
                       enum AVCodecID codec_id)
{
    AVCodecContext *c;
    int i;

    /* find the encoder */
    *codec = avcodec_find_encoder(codec_id);
    if (!(*codec)) {
        fprintf(stderr, "Could not find encoder for '%s'\n",
                avcodec_get_name(codec_id));
        exit(1);
    }

    ost->tmp_pkt = av_packet_alloc();
    if (!ost->tmp_pkt) {
        fprintf(stderr, "Could not allocate AVPacket\n");
        exit(1);
    }

    ost->st = avformat_new_stream(oc, NULL);
    if (!ost->st) {
        fprintf(stderr, "Could not allocate stream\n");
        exit(1);
    }
    ost->st->id = oc->nb_streams-1;
    c = avcodec_alloc_context3(*codec);
    if (!c) {
        fprintf(stderr, "Could not alloc an encoding context\n");
        exit(1);
    }
    ost->enc = c;

    switch ((*codec)->type) {
    case AVMEDIA_TYPE_AUDIO:
        c->sample_fmt  = (*codec)->sample_fmts ?
            (*codec)->sample_fmts[0] : AV_SAMPLE_FMT_FLTP;
        c->bit_rate    = 64000;
        c->sample_rate = 44100;
        if ((*codec)->supported_samplerates) {
            c->sample_rate = (*codec)->supported_samplerates[0];
            for (i = 0; (*codec)->supported_samplerates[i]; i++) {
                if ((*codec)->supported_samplerates[i] == 44100)
                    c->sample_rate = 44100;
            }
        }
        av_channel_layout_copy(&c->ch_layout, &(AVChannelLayout)AV_CHANNEL_LAYOUT_STEREO);
        ost->st->time_base = (AVRational){ 1, c->sample_rate };
        break;

    case AVMEDIA_TYPE_VIDEO:
        c->codec_id = codec_id;

        c->bit_rate = 16000000; // High intensity graphics look bad with lower bit-rates
        /* Resolution must be a multiple of two. */
        c->width    = canvas_width;
        c->height   = canvas_height;
        /* timebase: This is the fundamental unit of time (in seconds) in terms
         * of which frame timestamps are represented. For fixed-fps content,
         * timebase should be 1/framerate and timestamp increments should be
         * identical to 1. */
        ost->st->time_base = (AVRational){ 1, STREAM_FRAME_RATE };
        c->time_base       = ost->st->time_base;

        c->gop_size      = 12; /* emit one intra frame every twelve frames at most */
        c->pix_fmt       = STREAM_PIX_FMT;
        if (c->codec_id == AV_CODEC_ID_MPEG2VIDEO) {
            /* just for testing, we also add B-frames */
            c->max_b_frames = 2;
        }
        if (c->codec_id == AV_CODEC_ID_MPEG1VIDEO) {
            /* Needed to avoid using macroblocks in which some coeffs overflow.
             * This does not happen with normal video, it just happens here as
             * the motion of the chroma plane does not match the luma plane. */
            c->mb_decision = 2;
        }
        break;

    default:
        break;
    }

    /* Some formats want stream headers to be separate. */
    if (oc->oformat->flags & AVFMT_GLOBALHEADER)
        c->flags |= AV_CODEC_FLAG_GLOBAL_HEADER;
}

/**************************************************************/
/* audio output */

static AVFrame *alloc_audio_frame(enum AVSampleFormat sample_fmt,
                                  const AVChannelLayout *channel_layout,
                                  int sample_rate, int nb_samples)
{
    AVFrame *frame = av_frame_alloc();
    if (!frame) {
        fprintf(stderr, "Error allocating an audio frame\n");
        exit(1);
    }

    frame->format = sample_fmt;
    av_channel_layout_copy(&frame->ch_layout, channel_layout);
    frame->sample_rate = sample_rate;
    frame->nb_samples = nb_samples;

    if (nb_samples) {
        if (av_frame_get_buffer(frame, 0) < 0) {
            fprintf(stderr, "Error allocating an audio buffer\n");
            exit(1);
        }
    }

    return frame;
}

static void open_audio(AVFormatContext *oc, const AVCodec *codec,
                       OutputStream *ost, AVDictionary *opt_arg)
{
    AVCodecContext *c;
    int nb_samples;
    int ret;
    AVDictionary *opt = NULL;

    c = ost->enc;

    /* open it */
    av_dict_copy(&opt, opt_arg, 0);
    ret = avcodec_open2(c, codec, &opt);
    av_dict_free(&opt);
    if (ret < 0) {
        fprintf(stderr, "Could not open audio codec: %s\n", av_err2str(ret));
        exit(1);
    }

    /* init signal generator */
    ost->t     = 0;
    ost->tincr = 2 * M_PI * 110.0 / c->sample_rate;
    /* increment frequency by 110 Hz per second */
    ost->tincr2 = 2 * M_PI * 110.0 / c->sample_rate / c->sample_rate;

    if (c->codec->capabilities & AV_CODEC_CAP_VARIABLE_FRAME_SIZE)
        nb_samples = 10000;
    else
        nb_samples = c->frame_size;

    ost->frame     = alloc_audio_frame(c->sample_fmt, &c->ch_layout,
                                       c->sample_rate, nb_samples);
    ost->tmp_frame = alloc_audio_frame(AV_SAMPLE_FMT_S16, &c->ch_layout,
                                       c->sample_rate, nb_samples);

    /* copy the stream parameters to the muxer */
    ret = avcodec_parameters_from_context(ost->st->codecpar, c);
    if (ret < 0) {
        fprintf(stderr, "Could not copy the stream parameters\n");
        exit(1);
    }

    /* create resampler context */
    ost->swr_ctx = swr_alloc();
    if (!ost->swr_ctx) {
        fprintf(stderr, "Could not allocate resampler context\n");
        exit(1);
    }

    /* set options */
    av_opt_set_chlayout  (ost->swr_ctx, "in_chlayout",       &c->ch_layout,      0);
    av_opt_set_int       (ost->swr_ctx, "in_sample_rate",     c->sample_rate,    0);
    av_opt_set_sample_fmt(ost->swr_ctx, "in_sample_fmt",      AV_SAMPLE_FMT_S16, 0);
    av_opt_set_chlayout  (ost->swr_ctx, "out_chlayout",      &c->ch_layout,      0);
    av_opt_set_int       (ost->swr_ctx, "out_sample_rate",    c->sample_rate,    0);
    av_opt_set_sample_fmt(ost->swr_ctx, "out_sample_fmt",     c->sample_fmt,     0);

    /* initialize the resampling context */
    if ((ret = swr_init(ost->swr_ctx)) < 0) {
        fprintf(stderr, "Failed to initialize the resampling context\n");
        exit(1);
    }
}

/* Prepare a 16 bit dummy audio frame of 'frame_size' samples and
 * 'nb_channels' channels. */
static AVFrame *get_audio_frame(OutputStream *ost)
{
    AVFrame *frame = ost->tmp_frame;
    int j, i, v;
    int16_t *q = (int16_t*)frame->data[0];

    /* check if we want to generate more frames */
    if (av_compare_ts(ost->next_pts, ost->enc->time_base,
                      STREAM_DURATION, (AVRational){ 1, 1 }) > 0)
        return NULL;

    for (j = 0; j <frame->nb_samples; j++) {
        v = (int)(sin(ost->t) * 10000);
        for (i = 0; i < ost->enc->ch_layout.nb_channels; i++)
            *q++ = v;
        ost->t     += ost->tincr;
        ost->tincr += ost->tincr2;
    }

    frame->pts = ost->next_pts;
    ost->next_pts  += frame->nb_samples;

    return frame;
}

/*
 * encode one audio frame and send it to the muxer
 * return 1 when encoding is finished, 0 otherwise
 */
static int write_audio_frame(AVFormatContext *oc, OutputStream *ost)
{
    AVCodecContext *c;
    AVFrame *frame;
    int ret;
    int dst_nb_samples;

    c = ost->enc;

    frame = get_audio_frame(ost);

    if (frame) {
        /* convert samples from native format to destination codec format, using the resampler */
        /* compute destination number of samples */
        dst_nb_samples = swr_get_delay(ost->swr_ctx, c->sample_rate) + frame->nb_samples;
        av_assert0(dst_nb_samples == frame->nb_samples);

        /* when we pass a frame to the encoder, it may keep a reference to it
         * internally;
         * make sure we do not overwrite it here
         */
        ret = av_frame_make_writable(ost->frame);
        if (ret < 0)
            exit(1);

        /* convert to destination format */
        ret = swr_convert(ost->swr_ctx,
                          ost->frame->data, dst_nb_samples,
                          (const uint8_t **)frame->data, frame->nb_samples);
        if (ret < 0) {
            fprintf(stderr, "Error while converting\n");
            exit(1);
        }
        frame = ost->frame;

        frame->pts = av_rescale_q(ost->samples_count, (AVRational){1, c->sample_rate}, c->time_base);
        ost->samples_count += dst_nb_samples;
    }

    return write_frame(oc, c, ost->st, frame, ost->tmp_pkt);
}

/**************************************************************/
/* video output */

static AVFrame *alloc_frame(enum AVPixelFormat pix_fmt, int width, int height)
{
    AVFrame *frame;
    int ret;

    frame = av_frame_alloc();
    if (!frame)
        return NULL;

    frame->format = pix_fmt;
    frame->width  = width;
    frame->height = height;

    /* allocate the buffers for the frame data */
    ret = av_frame_get_buffer(frame, 0);
    if (ret < 0) {
        fprintf(stderr, "Could not allocate frame data.\n");
        exit(1);
    }

    return frame;
}

static void open_video(AVFormatContext *oc, const AVCodec *codec,
                       OutputStream *ost, AVDictionary *opt_arg)
{
    int ret;
    AVCodecContext *c = ost->enc;
    AVDictionary *opt = NULL;

    av_dict_copy(&opt, opt_arg, 0);

    /* open the codec */
    ret = avcodec_open2(c, codec, &opt);
    av_dict_free(&opt);
    if (ret < 0) {
        fprintf(stderr, "Could not open video codec: %s\n", av_err2str(ret));
        exit(1);
    }

    /* allocate and init a re-usable frame */
    ost->frame = alloc_frame(c->pix_fmt, c->width, c->height);
    if (!ost->frame) {
        fprintf(stderr, "Could not allocate video frame\n");
        exit(1);
    }

    /* If the output format is not YUV420P, then a temporary YUV420P
     * picture is needed too. It is then converted to the required
     * output format. */
    ost->tmp_frame = NULL;
    if (c->pix_fmt == AV_PIX_FMT_YUV420P) {
        ost->tmp_frame = alloc_frame(AV_PIX_FMT_RGB24, c->width, c->height);
        if (!ost->tmp_frame) {
            fprintf(stderr, "Could not allocate temporary video frame\n");
            exit(1);
        }
    }
	if (c->pix_fmt != AV_PIX_FMT_YUV420P) {
        ost->tmp_frame = alloc_frame(AV_PIX_FMT_YUV420P, c->width, c->height);
        if (!ost->tmp_frame) {
            fprintf(stderr, "Could not allocate temporary video frame\n");
            exit(1);
        }
    }

    /* copy the stream parameters to the muxer */
    ret = avcodec_parameters_from_context(ost->st->codecpar, c);
    if (ret < 0) {
        fprintf(stderr, "Could not copy the stream parameters\n");
        exit(1);
    }
}
`;

void fill_yuv_image(c:AVFrame ^pict, int frame_index, int width, int height)
{
	if (export_state.frames_written < export_state.total_frames) {
		c:c:`
		//pthread_mutex_lock(&images_lock);
		//while (images.size <= 0) {
		//	pthread_cond_wait(&more, &images_lock);
		//}
		`;
		Image img = images.remove_at(0);
		c:c:`
		//pthread_cond_signal(&less);
		//pthread_mutex_unlock(&images_lock);
		`;
		defer img.Unload();

		for (int y in 0..height) {
			for (int x in 0..width) {
				Color pixel = img.getXY(x, y);
				c:c:`
				pict->data[0][y * pict->linesize[0] + x * 3 + 0] = pixel.r;
				pict->data[0][y * pict->linesize[0] + x * 3 + 1] = pixel.g;
				pict->data[0][y * pict->linesize[0] + x * 3 + 2] = pixel.b;
				`;
			}
		}
		
		export_state.frames_written++;
	}
}

c:c:`

static AVFrame *get_video_frame(OutputStream *ost)
{
    AVCodecContext *c = ost->enc;

    /* check if we want to generate more frames */
    if (av_compare_ts(ost->next_pts, c->time_base,
                      STREAM_DURATION, (AVRational){ 1, 1 }) > 0)
        return NULL;

    /* when we pass a frame to the encoder, it may keep a reference to it
     * internally; make sure we do not overwrite it here */
    if (av_frame_make_writable(ost->frame) < 0)
        exit(1);

    if (c->pix_fmt != AV_PIX_FMT_YUV420P) {
		printf("We don't use this, something is wrong.\n");
		exit(1);
        /* as we only generate a YUV420P picture, we must convert it
         * to the codec pixel format if needed */
        if (!ost->sws_ctx) {
            ost->sws_ctx = sws_getContext(c->width, c->height,
                                          AV_PIX_FMT_YUV420P,
                                          c->width, c->height,
                                          c->pix_fmt,
                                          SCALE_FLAGS, NULL, NULL, NULL);
            if (!ost->sws_ctx) {
                fprintf(stderr,
                        "Could not initialize the conversion context\n");
                exit(1);
            }
        }
        fill_yuv_image(ost->tmp_frame, ost->next_pts, c->width, c->height);
        sws_scale(ost->sws_ctx, (const uint8_t * const *) ost->tmp_frame->data,
                  ost->tmp_frame->linesize, 0, c->height, ost->frame->data,
                  ost->frame->linesize);
    } else {
		struct SwsContext *sws_ctx = sws_getContext(
			c->width, c->height, AV_PIX_FMT_RGB24,
			c->width, c->height, AV_PIX_FMT_YUV420P,
			SWS_BILINEAR, NULL, NULL, NULL
		);
		if (!sws_ctx) {
			exit(1);
		}
        fill_yuv_image(ost->tmp_frame, ost->next_pts, c->width, c->height);
		sws_scale(sws_ctx, (const uint8_t * const*) ost->tmp_frame->data, ost->tmp_frame->linesize, 0, c->height, ost->frame->data, ost->frame->linesize);
    }

    ost->frame->pts = ost->next_pts++;

    return ost->frame;
}

/*
 * encode one video frame and send it to the muxer
 * return 1 when encoding is finished, 0 otherwise
 */
static int write_video_frame(AVFormatContext *oc, OutputStream *ost)
{
    return write_frame(oc, ost->enc, ost->st, get_video_frame(ost), ost->tmp_pkt);
}

static void close_stream(AVFormatContext *oc, OutputStream *ost)
{
    avcodec_free_context(&ost->enc);
    av_frame_free(&ost->frame);
    av_frame_free(&ost->tmp_frame);
    av_packet_free(&ost->tmp_pkt);
    sws_freeContext(ost->sws_ctx);
    swr_free(&ost->swr_ctx);
}

/***************************************************/
/* DECODING */

static AVFormatContext *fmt_ctx = NULL;
static AVCodecContext *video_dec_ctx = NULL, *audio_dec_ctx;
static int width, height;
static enum AVPixelFormat pix_fmt;
static AVStream *video_stream = NULL, *audio_stream = NULL;
static const char *src_filename = NULL;
static const char *video_dst_filename = NULL;
static const char *audio_dst_filename = NULL;
static FILE *video_dst_file = NULL;
static FILE *audio_dst_file = NULL;

static uint8_t *video_dst_data[4] = {NULL};
static int      video_dst_linesize[4];
static int video_dst_bufsize;

static int video_stream_idx = -1, audio_stream_idx = -1;
static AVFrame *frame = NULL;
static AVPacket *pkt = NULL;
static int video_frame_count = 0;
static int audio_frame_count = 0;
`;

void fill_rl_image(c:AVFrame^ pict, int width, int height)
{
	Image img = c:GenImageColor(width, height, c:BLACK);
	c:c:`
	struct SwsContext *sws_ctx = NULL;
	AVFrame* rgb_frame = av_frame_alloc();
	rgb_frame->format = AV_PIX_FMT_RGB24;
    rgb_frame->width  = width;
    rgb_frame->height = height;
   
    sws_ctx = sws_getContext(
        pict->width, pict->height, pix_fmt,
        width, height, AV_PIX_FMT_RGB24,
        SWS_BICUBIC,
        NULL, NULL, NULL
    );
    if (!sws_ctx) {
        fprintf(stderr, "Error initializing the sws context\n");
        exit(1);
    }

    // Set the new dimensions for the RGB frame
    if (av_image_alloc(rgb_frame->data, rgb_frame->linesize, width, height, AV_PIX_FMT_RGB24, 1) < 0) {
        fprintf(stderr, "Error allocating RGB frame\n");
        exit(1);
    }

    sws_scale(sws_ctx, (const uint8_t * const *) pict->data, pict->linesize, 0, pict->height, rgb_frame->data, rgb_frame->linesize);

    sws_freeContext(sws_ctx);
	`;

	for (int y in 0..height) {
		for (int x in 0..width) {
			c:c:`
			Color pixel = {
				.r = rgb_frame->data[0][y * rgb_frame->linesize[0] + x * 3 + 0],
				.g = rgb_frame->data[0][y * rgb_frame->linesize[0] + x * 3 + 1],
				.b = rgb_frame->data[0][y * rgb_frame->linesize[0] + x * 3 + 2],
				.a = 255
			};
			;`
			img.setXY(x, y, c:pixel);
		}
	}
	imported_images.add(img);
	c:c:`
	av_freep(&rgb_frame->data[0]);  // Free the data
	av_frame_free(&rgb_frame);       // Free the frame
	`;
	
}

c:c:`

static int output_video_frame(AVFrame *frame)
{
    if (frame->width != width || frame->height != height ||
        frame->format != pix_fmt) {
        /* To handle this change, one could call av_image_alloc again and
         * decode the following frames into another rawvideo file. */
        fprintf(stderr, "Error: Width, height and pixel format have to be "
                "constant in a rawvideo file, but the width, height or "
                "pixel format of the input video changed:\n"
                "old: width = %d, height = %d, format = %s\n"
                "new: width = %d, height = %d, format = %s\n",
                width, height, av_get_pix_fmt_name(pix_fmt),
                frame->width, frame->height,
                av_get_pix_fmt_name(frame->format));
        return -1;
    }

    printf("video_frame n:%d\n",
           video_frame_count++);

    /* copy decoded frame to destination buffer:
     * this is required since rawvideo expects non aligned data */
    av_image_copy2(video_dst_data, video_dst_linesize,
                   frame->data, frame->linesize,
                   pix_fmt, width, height);
				   
	fill_rl_image(frame, width, height);

    /* write to rawvideo file */
    fwrite(video_dst_data[0], 1, video_dst_bufsize, video_dst_file);
    return 0;
}

static int output_audio_frame(AVFrame *frame)
{
    size_t unpadded_linesize = frame->nb_samples * av_get_bytes_per_sample(frame->format);
    printf("audio_frame n:%d nb_samples:%d pts:%s\n",
           audio_frame_count++, frame->nb_samples,
           av_ts2timestr(frame->pts, &audio_dec_ctx->time_base));

    /* Write the raw audio data samples of the first plane. This works
     * fine for packed formats (e.g. AV_SAMPLE_FMT_S16). However,
     * most audio decoders output planar audio, which uses a separate
     * plane of audio samples for each channel (e.g. AV_SAMPLE_FMT_S16P).
     * In other words, this code will write only the first audio channel
     * in these cases.
     * You should use libswresample or libavfilter to convert the frame
     * to packed data. */
    fwrite(frame->extended_data[0], 1, unpadded_linesize, audio_dst_file);

    return 0;
}

static int decode_packet(AVCodecContext *dec, const AVPacket *pkt)
{
    int ret = 0;

    // submit the packet to the decoder
    ret = avcodec_send_packet(dec, pkt);
    if (ret < 0) {
        fprintf(stderr, "Error submitting a packet for decoding (%s)\n", av_err2str(ret));
        return ret;
    }

    // get all the available frames from the decoder
    while (ret >= 0) {
        ret = avcodec_receive_frame(dec, frame);
        if (ret < 0) {
            // those two return values are special and mean there is no output
            // frame available, but there were no errors during decoding
            if (ret == AVERROR_EOF || ret == AVERROR(EAGAIN))
                return 0;

            fprintf(stderr, "Error during decoding (%s)\n", av_err2str(ret));
            return ret;
        }

        // write the frame data to output file
        if (dec->codec->type == AVMEDIA_TYPE_VIDEO)
            ret = output_video_frame(frame);
        else
            ret = output_audio_frame(frame);

        av_frame_unref(frame);
    }

    return ret;
}

static int open_codec_context(int *stream_idx,
                              AVCodecContext **dec_ctx, AVFormatContext *fmt_ctx, enum AVMediaType type)
{
    int ret, stream_index;
    AVStream *st;
    const AVCodec *dec = NULL;

    ret = av_find_best_stream(fmt_ctx, type, -1, -1, NULL, 0);
    if (ret < 0) {
        fprintf(stderr, "Could not find %s stream in input file '%s'\n",
                av_get_media_type_string(type), src_filename);
        return ret;
    } else {
        stream_index = ret;
        st = fmt_ctx->streams[stream_index];

        /* find decoder for the stream */
        dec = avcodec_find_decoder(st->codecpar->codec_id);
        if (!dec) {
            fprintf(stderr, "Failed to find %s codec\n",
                    av_get_media_type_string(type));
            return AVERROR(EINVAL);
        }

        /* Allocate a codec context for the decoder */
        *dec_ctx = avcodec_alloc_context3(dec);
        if (!*dec_ctx) {
            fprintf(stderr, "Failed to allocate the %s codec context\n",
                    av_get_media_type_string(type));
            return AVERROR(ENOMEM);
        }

        /* Copy codec parameters from input stream to output codec context */
        if ((ret = avcodec_parameters_to_context(*dec_ctx, st->codecpar)) < 0) {
            fprintf(stderr, "Failed to copy %s codec parameters to decoder context\n",
                    av_get_media_type_string(type));
            return ret;
        }

        /* Init the decoders */
        if ((ret = avcodec_open2(*dec_ctx, dec, NULL)) < 0) {
            fprintf(stderr, "Failed to open %s codec\n",
                    av_get_media_type_string(type));
            return ret;
        }
        *stream_idx = stream_index;
    }

    return 0;
}

static int get_format_from_sample_fmt(const char **fmt,
                                      enum AVSampleFormat sample_fmt)
{
    int i;
    struct sample_fmt_entry {
        enum AVSampleFormat sample_fmt; const char *fmt_be, *fmt_le;
    } sample_fmt_entries[] = {
        { AV_SAMPLE_FMT_U8,  "u8",    "u8"    },
        { AV_SAMPLE_FMT_S16, "s16be", "s16le" },
        { AV_SAMPLE_FMT_S32, "s32be", "s32le" },
        { AV_SAMPLE_FMT_FLT, "f32be", "f32le" },
        { AV_SAMPLE_FMT_DBL, "f64be", "f64le" },
    };
    *fmt = NULL;

    for (i = 0; i < FF_ARRAY_ELEMS(sample_fmt_entries); i++) {
        struct sample_fmt_entry *entry = &sample_fmt_entries[i];
        if (sample_fmt == entry->sample_fmt) {
            *fmt = AV_NE(entry->fmt_be, entry->fmt_le);
            return 0;
        }
    }

    fprintf(stderr,
            "sample format %s is not supported as output format\n",
            av_get_sample_fmt_name(sample_fmt));
    return -1;
}
`;

//===============================================================================

bool ImportVideo(int framerate, char^ folder_path, char^ input_file_name_no_path) {
	c:c:`
	int ret = 0;

    // if (argc != 4) {
    //     fprintf(stderr, "usage: %s  input_file video_output_file audio_output_file\n"
    //             "API example program to show how to read frames from an input file.\n"
    //             "This program reads frames from a file, decodes them, and writes decoded\n"
    //             "video frames to a rawvideo file named video_output_file, and decoded\n"
    //             "audio frames to a rawaudio file named audio_output_file.\n",
    //             argv[0]);
    //     exit(1);
    // }
    src_filename = input_file_name_no_path;
    video_dst_filename = "video.raw";
    audio_dst_filename = "audio.raw";

    /* open input file, and allocate format context */
    if (avformat_open_input(&fmt_ctx, src_filename, NULL, NULL) < 0) {
        fprintf(stderr, "Could not open source file %s\n", src_filename);
        exit(1);
    }

    /* retrieve stream information */
    if (avformat_find_stream_info(fmt_ctx, NULL) < 0) {
        fprintf(stderr, "Could not find stream information\n");
        exit(1);
    }

    if (open_codec_context(&video_stream_idx, &video_dec_ctx, fmt_ctx, AVMEDIA_TYPE_VIDEO) >= 0) {
        video_stream = fmt_ctx->streams[video_stream_idx];

        video_dst_file = fopen(video_dst_filename, "wb");
        if (!video_dst_file) {
            fprintf(stderr, "Could not open destination file %s\n", video_dst_filename);
            ret = 1;
            goto end;
        }

        /* allocate image where the decoded image will be put */
        width = video_dec_ctx->width;
        height = video_dec_ctx->height;
        pix_fmt = video_dec_ctx->pix_fmt;
        ret = av_image_alloc(video_dst_data, video_dst_linesize,
                             width, height, pix_fmt, 1);
        if (ret < 0) {
            fprintf(stderr, "Could not allocate raw video buffer\n");
            goto end;
        }
        video_dst_bufsize = ret;
    }

    if (open_codec_context(&audio_stream_idx, &audio_dec_ctx, fmt_ctx, AVMEDIA_TYPE_AUDIO) >= 0) {
        audio_stream = fmt_ctx->streams[audio_stream_idx];
        audio_dst_file = fopen(audio_dst_filename, "wb");
        if (!audio_dst_file) {
            fprintf(stderr, "Could not open destination file %s\n", audio_dst_filename);
            ret = 1;
            goto end;
        }
    }

    /* dump input information to stderr */
    av_dump_format(fmt_ctx, 0, src_filename, 0);

    if (!audio_stream && !video_stream) {
        fprintf(stderr, "Could not find audio or video stream in the input, aborting\n");
        ret = 1;
        goto end;
    }

    frame = av_frame_alloc();
    if (!frame) {
        fprintf(stderr, "Could not allocate frame\n");
        ret = AVERROR(ENOMEM);
        goto end;
    }

    pkt = av_packet_alloc();
    if (!pkt) {
        fprintf(stderr, "Could not allocate packet\n");
        ret = AVERROR(ENOMEM);
        goto end;
    }

    if (video_stream)
        printf("Demuxing video from file '%s' into '%s'\n", src_filename, video_dst_filename);
    if (audio_stream)
        printf("Demuxing audio from file '%s' into '%s'\n", src_filename, audio_dst_filename);

    /* read frames from the file */
    while (av_read_frame(fmt_ctx, pkt) >= 0) {
        // check if the packet belongs to a stream we are interested in, otherwise
        // skip it
        if (pkt->stream_index == video_stream_idx)
            ret = decode_packet(video_dec_ctx, pkt);
        else if (pkt->stream_index == audio_stream_idx)
            ret = decode_packet(audio_dec_ctx, pkt);
        av_packet_unref(pkt);
        if (ret < 0)
            break;
    }

    /* flush the decoders */
    if (video_dec_ctx)
        decode_packet(video_dec_ctx, NULL);
    if (audio_dec_ctx)
        decode_packet(audio_dec_ctx, NULL);

    printf("Demuxing succeeded.\n");

    if (video_stream) {
        printf("Play the output video file with the command:\n"
               "ffplay -f rawvideo -pixel_format %s -video_size %dx%d %s\n",
               av_get_pix_fmt_name(pix_fmt), width, height,
               video_dst_filename);
    }

    if (audio_stream) {
        enum AVSampleFormat sfmt = audio_dec_ctx->sample_fmt;
        int n_channels = audio_dec_ctx->ch_layout.nb_channels;
        const char *fmt;

        if (av_sample_fmt_is_planar(sfmt)) {
            const char *packed = av_get_sample_fmt_name(sfmt);
            printf("Warning: the sample format the decoder produced is planar "
                   "(%s). This example will output the first channel only.\n",
                   packed ? packed : "?");
            sfmt = av_get_packed_sample_fmt(sfmt);
            n_channels = 1;
        }

        if ((ret = get_format_from_sample_fmt(&fmt, sfmt)) < 0)
            goto end;

		char* channel_type;
		if (n_channels == 1) {
			channel_type = "mono";
		} else {
			channel_type = "stereo";
		}
        printf("Play the output audio file with the command:\n"
               "ffplay -f %s -ch_layout %s -ar %d %s\n",
               fmt, channel_type, audio_dec_ctx->sample_rate,
               audio_dst_filename);
    }

end:
    avcodec_free_context(&video_dec_ctx);
    avcodec_free_context(&audio_dec_ctx);
    avformat_close_input(&fmt_ctx);
    if (video_dst_file)
        fclose(video_dst_file);
    if (audio_dst_file)
        fclose(audio_dst_file);
    av_packet_free(&pkt);
    av_frame_free(&frame);
    av_free(video_dst_data[0]);

	`;
	// Check that imported_images is populated
	// while (!imported_images.is_empty()) {
	// 	Image img = imported_images.pop_back();
	// 	defer(img.Unload());
	// 	// Texture tex = c:LoadTextureFromImage(img);
	// 	char^ name = t"import_out/{imported_images.size %d05}.png";
	// 	c:ExportImage(img, name);
	// }
	return false;
}


void ExportVideo(int framerate, char^ folder_path, char^ output_file_name_no_path) {
	Image img = c:LoadImageFromTexture(canvas.texture);
	int imgh = img.height;
	int imgw = img.width;

	char ^filename;
	filename = str_concat(output_file_name_no_path, ".mp4");

	c:c:`
	OutputStream video_st = { 0 }, audio_st = { 0 };
    const AVOutputFormat *fmt;
    AVFormatContext *oc;
    const AVCodec *audio_codec, *video_codec;
    int ret;
    int have_video = 0, have_audio = 0;
    int encode_video = 0, encode_audio = 0;
    AVDictionary *opt = NULL;
    int i;

    /* allocate the output media context */
    avformat_alloc_output_context2(&oc, NULL, NULL, filename);
    if (!oc) {
        printf("Could not deduce output format from file extension: using MPEG.\n");
        avformat_alloc_output_context2(&oc, NULL, "mpeg", filename);
		return;
    }

    fmt = oc->oformat;

    /* Add the audio and video streams using the default format codecs
     * and initialize the codecs. */
    if (fmt->video_codec != AV_CODEC_ID_NONE) {
        add_stream(&video_st, oc, &video_codec, fmt->video_codec);
        have_video = 1;
        encode_video = 1;
    }
    if (fmt->audio_codec != AV_CODEC_ID_NONE) {
        add_stream(&audio_st, oc, &audio_codec, fmt->audio_codec);
        have_audio = 1;
        encode_audio = 1;
    }

    /* Now that all the parameters are set, we can open the audio and
     * video codecs and allocate the necessary encode buffers. */
    if (have_video)
        open_video(oc, video_codec, &video_st, opt);

    if (have_audio)
        open_audio(oc, audio_codec, &audio_st, opt);

    av_dump_format(oc, 0, filename, 1);

    /* open the output file, if needed */
    if (!(fmt->flags & AVFMT_NOFILE)) {
        ret = avio_open(&oc->pb, filename, AVIO_FLAG_WRITE);
        if (ret < 0) {
            fprintf(stderr, "Could not open '%s': %s\n", filename,
                    av_err2str(ret));
            return;
        }
    }

    /* Write the stream header, if any. */
    ret = avformat_write_header(oc, &opt);
    if (ret < 0) {
        fprintf(stderr, "Error occurred when opening output file: %s\n",
                av_err2str(ret));
        return;
    }

    while (encode_video || encode_audio) {
        /* select the stream to encode */
        if (encode_video &&
            (!encode_audio || av_compare_ts(video_st.next_pts, video_st.enc->time_base,
                                            audio_st.next_pts, audio_st.enc->time_base) <= 0)) {
            encode_video = !write_video_frame(oc, &video_st);
        } else {
            encode_audio = !write_audio_frame(oc, &audio_st);
        }
    }

    av_write_trailer(oc);

    /* Close each codec. */
    if (have_video)
        close_stream(oc, &video_st);
    if (have_audio)
        close_stream(oc, &audio_st);	

    if (!(fmt->flags & AVFMT_NOFILE))
        /* Close the output file. */
        avio_closep(&oc->pb);

    /* free the stream */
    avformat_free_context(oc);
	`;
	SetMode(.Running);
}

void OnFileDropped() {
	FilePathList dropped_file_path_list = FilePathList.Load();
	defer dropped_file_path_list.Unload();
	
	for (int i in 0..dropped_file_path_list.count) {
		char^ file_type = c:GetFileExtension(dropped_file_path_list.paths[i]);
		char^ file_path = strdup(dropped_file_path_list.paths[i]);
		// malloced image (the ImageElement is responsible for freeing)
		// ^ TODO: make this ownership clearer!
		println(t"File dropped: {strcmp(file_type, ".png")}");
		if (strcmp(file_type, ".png") == 0 || strcmp(file_type, ".gif") == 0 || strcmp(file_type, ".jpg") == 0) {
			// Handle image file
			Image img = Image.Load(file_path);
			defer img.Unload();
			char^ img_name = c:GetFileNameWithoutExt(file_path);
			AddNewElementAt(Element(ImageElement.Make(file_path), strdup(img_name), current_time, new_element_default_duration, -1, mouse.GetPos(), v2(img.width, img.height)));
			SelectNewestElement();

		} else if (strcmp(file_type, ".csv") == 0) {
			// Handle data file
			Data data = Data(file_path);
			data_list.add(data);
			elements.get(selected_elem_i).ApplyKeyframeData(data, current_time, (1.0 / frame_rate));
		} else {
			println(t"Unsupported file type: {file_type}");
		}

	}
}

Rectangle UnCoveredArea() {
	if (ui_hidden) {
		return .(0, 0, window_width, window_height);
	}

	return .(left_panel_width, 0, window_width - left_panel_width, window_height - (element_timeline_ui_height));
}

Rectangle CanvasRect() -> UnCoveredArea().Inset(ui_hidden ? 0 | 10).FitIntoSelfWithAspectRatio(canvas_width, canvas_height);
